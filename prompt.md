# ðŸ§  MASTER PROMPT â€” Jira AI Co-pilot MVP

> **Destinataire** : Claude Opus 4.5 (Vibe Coding Mode)
> **Auteur** : Product Owner / CTO
> **Version** : 1.0
> **DurÃ©e projet** : 12 semaines (6 sprints Ã— 2 semaines)

---

## PARTIE 1 : CONTEXTE GLOBAL

### 1.1 IdentitÃ© du projet

**Nom** : Jira AI Co-pilot
**Nature** : Outil d'intelligence dÃ©cisionnelle personnel branchÃ© sur Jira
**Philosophie** : Ne pas refaire Jira, mais ajouter une couche d'IA prÃ©dictive par-dessus

### 1.2 Situation initiale

Je suis un dÃ©veloppeur/manager qui gÃ¨re un projet IT sur Jira Cloud. Mon instance contient :
- **Volume** : Centaines de tickets actifs
- **Ã‰quipe** : Dizaines de dÃ©veloppeurs
- **MÃ©thodologie** : Scrum/Kanban avec sprints, story points, worklogs

**Mes frustrations actuelles** :
- Je dÃ©couvre les retards trop tard, quand le sprint est dÃ©jÃ  compromis
- La charge de travail entre dÃ©veloppeurs est dÃ©sÃ©quilibrÃ©e â€” certains sont surchargÃ©s, d'autres sous-utilisÃ©s
- Les estimations de tickets sont souvent fausses â€” je n'ai aucune aide data-driven
- Organiser les releases et prioriser le backlog me prend un temps fou
- Je n'ai pas de visibilitÃ© sur les risques de burn-out dans l'Ã©quipe

### 1.3 Vision du produit

Un assistant IA local qui :
1. **Anticipe** les problÃ¨mes avant qu'ils n'arrivent (retards, surcharge, blocages)
2. **SuggÃ¨re** des actions correctives (rÃ©assignation, re-priorisation)
3. **PrÃ©dit** les dÃ©lais de faÃ§on rÃ©aliste basÃ©e sur l'historique
4. **Optimise** l'organisation des releases et du backlog
5. **Dialogue** avec moi en langage naturel pour explorer les donnÃ©es

### 1.4 Contraintes techniques imposÃ©es

| Contrainte | Valeur | Raison |
|------------|--------|--------|
| **Langage** | Python 100% | Ã‰cosystÃ¨me ML, prÃ©fÃ©rence personnelle |
| **Infrastructure** | Local uniquement | MVP validation, pas de coÃ»ts cloud |
| **Base de donnÃ©es** | DuckDB | Performance analytique, zÃ©ro config |
| **LLM** | Ollama (Llama 3.1 8B ou Mistral 7B) | Gratuit, local, privacy |
| **API Jira** | REST API + sync incrÃ©mental | Robustesse, contrÃ´le total |
| **Interface** | CLI + Streamlit basique | Validation rapide, on s'en fout du design |
| **QualitÃ© graphique** | Minimale acceptable | Focus sur la valeur fonctionnelle |

### 1.5 Stack technique validÃ©e

```
INGESTION DONNÃ‰ES
â”œâ”€â”€ jira-python >= 3.10.0          # Client API Jira officiel
â”œâ”€â”€ tenacity >= 8.0.0              # Retry logic avec backoff
â””â”€â”€ apscheduler >= 3.11.0          # Orchestration syncs

STOCKAGE & ANALYTICS
â”œâ”€â”€ duckdb >= 1.0.0                # Base analytique columnaire
â”œâ”€â”€ pandas >= 2.0.0                # Manipulation dataframes
â””â”€â”€ pyarrow >= 15.0.0              # Interop performante

MACHINE LEARNING
â”œâ”€â”€ scikit-learn >= 1.4.0          # ModÃ¨les baseline
â”œâ”€â”€ lightgbm >= 4.0.0              # Gradient boosting (features catÃ©gorielles)
â””â”€â”€ optuna >= 3.6.0                # Hyperparameter tuning (optionnel)

LLM & NLP
â”œâ”€â”€ ollama >= 0.3.0                # Client Ollama local
â”œâ”€â”€ tiktoken >= 0.7.0              # Token counting
â””â”€â”€ sentence-transformers >= 3.0.0 # Embeddings (optionnel)

INTERFACE
â”œâ”€â”€ streamlit >= 1.35.0            # Dashboard rapide
â”œâ”€â”€ rich >= 13.0.0                 # CLI amÃ©liorÃ©e
â””â”€â”€ typer >= 0.12.0                # CLI structurÃ©e

UTILS
â”œâ”€â”€ pydantic-settings >= 2.0.0     # Config typÃ©e
â”œâ”€â”€ python-dotenv >= 1.0.0         # Variables environnement
â””â”€â”€ loguru >= 0.7.0                # Logging simplifiÃ©
```

### 1.6 Structure projet cible

```
jira-copilot/
â”œâ”€â”€ .env.example                   # Template variables environnement
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                       # Commandes principales (sync, train, predict, dashboard)
â”œâ”€â”€ pyproject.toml                 # DÃ©pendances + config
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py               # Pydantic settings (chargement .env)
â”‚   â”œâ”€â”€ jira_config.yaml          # Projets, custom fields mapping
â”‚   â””â”€â”€ model_config.yaml         # HyperparamÃ¨tres ML
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ jira.duckdb               # Base principale
â”‚   â”œâ”€â”€ raw/                      # JSON bruts (backup)
â”‚   â””â”€â”€ exports/                  # CSV/reports exportÃ©s
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ticket_estimator.pkl      # ModÃ¨le estimation dÃ©lais
â”‚   â”œâ”€â”€ sprint_risk.pkl           # ModÃ¨le risque sprint
â”‚   â””â”€â”€ metadata.json             # Versions, mÃ©triques, dates
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ jira_client/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ auth.py               # Authentification Jira
â”‚   â”‚   â”œâ”€â”€ fetcher.py            # RÃ©cupÃ©ration donnÃ©es
â”‚   â”‚   â”œâ”€â”€ rate_limiter.py       # Gestion rate limiting
â”‚   â”‚   â””â”€â”€ sync.py               # Orchestration sync incrÃ©mental
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema.py             # DÃ©finition tables DuckDB
â”‚   â”‚   â”œâ”€â”€ loader.py             # Chargement donnÃ©es brutes â†’ DuckDB
â”‚   â”‚   â””â”€â”€ queries.py            # Queries SQL rÃ©utilisables
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ticket_features.py    # Features pour estimation tickets
â”‚   â”‚   â”œâ”€â”€ developer_features.py # Features charge dÃ©veloppeur
â”‚   â”‚   â”œâ”€â”€ sprint_features.py    # Features santÃ© sprint
â”‚   â”‚   â””â”€â”€ pipeline.py           # Pipeline feature engineering
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ticket_estimator.py   # PrÃ©diction durÃ©e tickets
â”‚   â”‚   â”œâ”€â”€ sprint_risk.py        # Score risque retard sprint
â”‚   â”‚   â”œâ”€â”€ workload_scorer.py    # Score charge dÃ©veloppeur
â”‚   â”‚   â””â”€â”€ trainer.py            # Training + Ã©valuation
â”‚   â”‚
â”‚   â”œâ”€â”€ intelligence/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py         # Interface Ollama
â”‚   â”‚   â”œâ”€â”€ prompts.py            # Templates prompts
â”‚   â”‚   â”œâ”€â”€ analyst.py            # Analyse LLM des donnÃ©es
â”‚   â”‚   â””â”€â”€ recommender.py        # Suggestions actions
â”‚   â”‚
â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ load_balancer.py      # Suggestions rÃ©assignation
â”‚   â”‚   â”œâ”€â”€ release_planner.py    # GÃ©nÃ©ration plans release
â”‚   â”‚   â””â”€â”€ alert_generator.py    # GÃ©nÃ©ration alertes
â”‚   â”‚
â”‚   â””â”€â”€ interface/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cli.py                # Interface ligne de commande
â”‚       â””â”€â”€ dashboard.py          # Streamlit app
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py               # Fixtures pytest
â”‚   â”œâ”€â”€ test_jira_client/
â”‚   â”œâ”€â”€ test_features/
â”‚   â”œâ”€â”€ test_models/
â”‚   â””â”€â”€ test_intelligence/
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_exploration.ipynb      # Exploration donnÃ©es initiale
    â”œâ”€â”€ 02_feature_engineering.ipynb
    â””â”€â”€ 03_model_experiments.ipynb
```

### 1.7 DonnÃ©es Jira disponibles

**EntitÃ©s principales Ã  ingÃ©rer** :
- `Issues` : key, summary, description, type, status, priority, assignee, reporter, created, updated, resolved, story_points, components, labels, sprint, epic
- `Changelog` : timestamps de chaque transition de status (crucial pour cycle time)
- `Worklogs` : temps passÃ© par dÃ©veloppeur par ticket (attention RGPD)
- `Sprints` : name, state, startDate, endDate, goal
- `Users` : accountId, displayName, emailAddress (pseudonymiser)
- `Projects` : key, name, projectTypeKey

**MÃ©triques dÃ©rivÃ©es Ã  calculer** :
- `lead_time` : created â†’ resolved
- `cycle_time` : first "In Progress" â†’ resolved
- `time_in_status` : durÃ©e dans chaque status
- `velocity` : story points completed par sprint
- `scope_creep` : points ajoutÃ©s mid-sprint / points committed
- `completion_rate` : tickets done / tickets committed

### 1.8 ConsidÃ©rations RGPD

**RÃ¨gle d'or pour le MVP** : PrivilÃ©gier les mÃ©triques d'Ã©quipe agrÃ©gÃ©es, pas individuelles.

Si donnÃ©es individuelles nÃ©cessaires :
- Pseudonymiser les identifiants dÃ©veloppeurs (hash salÃ©)
- Ne jamais afficher de noms rÃ©els dans les logs
- PrÃ©voir un flag `--anonymize` pour tous les exports

---

## PARTIE 2 : OBJECTIF GLOBAL

### 2.1 Mission

DÃ©velopper un MVP fonctionnel en 12 semaines qui prouve la valeur d'un assistant IA pour la gestion de projet Jira, en validant 4 hypothÃ¨ses business :

| # | HypothÃ¨se | MÃ©trique de succÃ¨s |
|---|-----------|-------------------|
| H1 | L'IA peut prÃ©dire la durÃ©e des tickets mieux que les estimations humaines | MAE < estimation humaine de 15%+ |
| H2 | L'IA peut dÃ©tecter les sprints Ã  risque avant mi-sprint | PrÃ©cision alertes > 70% |
| H3 | L'IA peut identifier les dÃ©sÃ©quilibres de charge dans l'Ã©quipe | DÃ©tection Ã©carts > 30% de la moyenne |
| H4 | L'IA peut gÃ©nÃ©rer des plans de release cohÃ©rents | Temps de planification rÃ©duit de 50% |

### 2.2 DÃ©finition du "Done" MVP

Le MVP est validÃ© quand je peux :

1. **Synchroniser** mon projet Jira en une commande (`make sync`)
2. **Visualiser** un dashboard avec les mÃ©triques clÃ©s du sprint en cours
3. **Recevoir** une prÃ©diction de durÃ©e pour tout nouveau ticket
4. **Voir** un score de risque pour le sprint actif avec les raisons
5. **Consulter** la charge de travail par dÃ©veloppeur (agrÃ©gÃ©e ou pseudonymisÃ©e)
6. **Dialoguer** avec le LLM pour poser des questions sur mes donnÃ©es
7. **GÃ©nÃ©rer** une proposition de plan de release basÃ©e sur le backlog

### 2.3 Ce qui est OUT OF SCOPE pour le MVP

- Interface web production-ready (Streamlit basique suffit)
- Multi-tenant / multi-utilisateurs
- DÃ©ploiement cloud
- IntÃ©gration bidirectionnelle (Ã©criture dans Jira)
- Notifications automatiques (Slack, email)
- Authentification utilisateur
- Tests de charge / performance
- Documentation utilisateur complÃ¨te

---

## PARTIE 3 : ROADMAP DÃ‰TAILLÃ‰E â€” 6 SPRINTS

---

### ðŸƒ SPRINT 1 : Fondations & Pipeline de donnÃ©es
**Semaines 1-2**

#### Objectif Sprint
Ã‰tablir une connexion fiable avec Jira et construire le pipeline d'ingestion de donnÃ©es complet. Ã€ la fin de ce sprint, toutes les donnÃ©es nÃ©cessaires sont dans DuckDB et requÃªtables.

#### User Stories

**US1.1 â€” Configuration projet**
> En tant que dÃ©veloppeur, je veux initialiser le projet avec toute la structure et les dÃ©pendances pour pouvoir commencer Ã  coder immÃ©diatement.

CritÃ¨res d'acceptation :
- [ ] `pyproject.toml` avec toutes les dÃ©pendances listÃ©es dans la stack
- [ ] Structure de dossiers conforme au template dÃ©fini
- [ ] `.env.example` documentÃ© avec toutes les variables nÃ©cessaires
- [ ] `Makefile` avec commandes : `install`, `test`, `lint`
- [ ] `.gitignore` appropriÃ© (inclut `.env`, `*.duckdb`, `models/*.pkl`)
- [ ] README avec instructions de setup

**US1.2 â€” Authentification Jira**
> En tant que dÃ©veloppeur, je veux me connecter Ã  mon instance Jira Cloud de faÃ§on sÃ©curisÃ©e.

CritÃ¨res d'acceptation :
- [ ] Module `auth.py` avec classe `JiraAuthenticator`
- [ ] Support authentification Basic Auth (email + API token)
- [ ] Validation de la connexion au dÃ©marrage (test endpoint `/myself`)
- [ ] Gestion propre des erreurs d'authentification
- [ ] Configuration via `.env` : `JIRA_URL`, `JIRA_EMAIL`, `JIRA_API_TOKEN`

**US1.3 â€” RÃ©cupÃ©ration des Issues**
> En tant que dÃ©veloppeur, je veux rÃ©cupÃ©rer tous les tickets d'un projet avec leur historique complet.

CritÃ¨res d'acceptation :
- [ ] Module `fetcher.py` avec classe `JiraFetcher`
- [ ] MÃ©thode `fetch_issues(project_key, jql_filter=None)` avec pagination automatique
- [ ] RÃ©cupÃ©ration des champs : key, summary, description, type, status, priority, assignee, reporter, created, updated, resolutiondate, story_points (custom field), components, labels, sprint, epic_link
- [ ] Expansion du changelog pour chaque issue
- [ ] Gestion de la pagination (maxResults=100, startAt incrÃ©mentÃ©)
- [ ] Sauvegarde JSON brut dans `data/raw/issues_{timestamp}.json`

**US1.4 â€” Rate Limiting intelligent**
> En tant que dÃ©veloppeur, je veux que le systÃ¨me respecte les limites de l'API Jira sans intervention manuelle.

CritÃ¨res d'acceptation :
- [ ] Module `rate_limiter.py` avec dÃ©corateur `@rate_limited`
- [ ] DÃ©tection du header `X-RateLimit-Remaining`
- [ ] Exponential backoff avec jitter sur erreur 429
- [ ] Retry automatique (max 3 tentatives) via `tenacity`
- [ ] Logging des Ã©vÃ©nements de rate limiting

**US1.5 â€” RÃ©cupÃ©ration donnÃ©es complÃ©mentaires**
> En tant que dÃ©veloppeur, je veux rÃ©cupÃ©rer les sprints, worklogs et utilisateurs.

CritÃ¨res d'acceptation :
- [ ] MÃ©thode `fetch_sprints(board_id)` â€” tous les sprints du board
- [ ] MÃ©thode `fetch_worklogs(since_timestamp)` â€” worklogs mis Ã  jour depuis date
- [ ] MÃ©thode `fetch_users(project_key)` â€” utilisateurs assignables au projet
- [ ] MÃ©thode `fetch_boards(project_key)` â€” boards associÃ©s au projet
- [ ] Sauvegarde JSON brut sÃ©parÃ©e pour chaque entitÃ©

**US1.6 â€” SchÃ©ma DuckDB**
> En tant que dÃ©veloppeur, je veux un schÃ©ma de base de donnÃ©es optimisÃ© pour l'analytique.

CritÃ¨res d'acceptation :
- [ ] Module `schema.py` avec fonction `initialize_database(db_path)`
- [ ] Tables : `issues`, `issue_changelog`, `sprints`, `worklogs`, `users`, `sync_metadata`
- [ ] Index sur : `issues.key`, `issues.assignee_id`, `issues.sprint_id`, `issue_changelog.issue_key`, `worklogs.issue_key`
- [ ] Table `sync_metadata` : derniÃ¨re sync par entitÃ©, nombre de records
- [ ] Types appropriÃ©s : TIMESTAMP pour dates, INTEGER pour IDs, VARCHAR pour texte

**US1.7 â€” Chargement donnÃ©es**
> En tant que dÃ©veloppeur, je veux charger les donnÃ©es JSON brutes dans DuckDB avec transformation.

CritÃ¨res d'acceptation :
- [ ] Module `loader.py` avec classe `DataLoader`
- [ ] MÃ©thode `load_issues(json_path)` : parsing + insertion/upsert
- [ ] MÃ©thode `load_changelog(issues_data)` : extraction changelog â†’ table dÃ©diÃ©e
- [ ] MÃ©thode `load_sprints(json_path)` 
- [ ] MÃ©thode `load_worklogs(json_path)`
- [ ] MÃ©thode `load_users(json_path)` avec pseudonymisation optionnelle
- [ ] Gestion des custom fields (mapping configurable dans `jira_config.yaml`)
- [ ] Idempotence : relancer le load ne crÃ©e pas de doublons

**US1.8 â€” Sync incrÃ©mental orchestrÃ©**
> En tant que dÃ©veloppeur, je veux synchroniser uniquement les donnÃ©es modifiÃ©es depuis la derniÃ¨re sync.

CritÃ¨res d'acceptation :
- [ ] Module `sync.py` avec classe `JiraSyncOrchestrator`
- [ ] MÃ©thode `full_sync()` : import initial complet
- [ ] MÃ©thode `incremental_sync()` : JQL `updated >= "{last_sync}"`
- [ ] Mise Ã  jour automatique de `sync_metadata` aprÃ¨s chaque sync
- [ ] Commande CLI : `python -m src.jira_client.sync --mode [full|incremental]`
- [ ] Ajout au Makefile : `make sync` (incrÃ©mental par dÃ©faut)

**US1.9 â€” Queries analytiques de base**
> En tant que dÃ©veloppeur, je veux des queries SQL rÃ©utilisables pour les analyses courantes.

CritÃ¨res d'acceptation :
- [ ] Module `queries.py` avec fonctions retournant des DataFrames
- [ ] `get_issues_with_metrics(project_key)` : issues + lead_time + cycle_time calculÃ©s
- [ ] `get_sprint_summary(sprint_id)` : committed, completed, added, removed
- [ ] `get_developer_workload(days=30)` : issues assignÃ©es, completed, en cours par dev
- [ ] `get_velocity_history(n_sprints=10)` : velocity par sprint

#### Livrables Sprint 1
- [ ] Pipeline de sync fonctionnel (`make sync`)
- [ ] Base DuckDB peuplÃ©e avec donnÃ©es rÃ©elles
- [ ] Notebook `01_exploration.ipynb` validant l'accÃ¨s aux donnÃ©es
- [ ] Tests unitaires pour `auth.py`, `fetcher.py`, `loader.py`

#### Orientations techniques Sprint 1

**Authentification** :
```python
# Pattern recommandÃ© pour jira-python
from jira import JIRA
jira = JIRA(server=url, basic_auth=(email, token))
```

**Pagination robuste** :
```python
# Boucle until exhaustion
while True:
    results = jira.search_issues(jql, startAt=start, maxResults=100)
    all_issues.extend(results)
    if len(results) < 100:
        break
    start += 100
```

**DuckDB connexion** :
```python
import duckdb
conn = duckdb.connect('data/jira.duckdb')
# Pandas interop natif
df = conn.sql("SELECT * FROM issues").df()
```

**Custom fields Jira** :
Les story points sont souvent dans un custom field (`customfield_10016`). PrÃ©voir un mapping configurable :
```yaml
# config/jira_config.yaml
custom_fields:
  story_points: customfield_10016
  epic_link: customfield_10014
  sprint: customfield_10020
```

---

### ðŸƒ SPRINT 2 : Feature Engineering & Premier ModÃ¨le
**Semaines 3-4**

#### Objectif Sprint
Construire le pipeline de feature engineering et entraÃ®ner le premier modÃ¨le prÃ©dictif (estimation durÃ©e tickets). Valider l'hypothÃ¨se H1.

#### User Stories

**US2.1 â€” Features tickets**
> En tant que data scientist, je veux des features pertinentes pour prÃ©dire la durÃ©e d'un ticket.

CritÃ¨res d'acceptation :
- [ ] Module `ticket_features.py` avec classe `TicketFeatureExtractor`
- [ ] Features numÃ©riques :
  - `story_points` : points estimÃ©s (si disponible)
  - `description_length` : nombre de caractÃ¨res description
  - `num_components` : nombre de composants taguÃ©s
  - `num_labels` : nombre de labels
  - `num_subtasks` : nombre de sous-tÃ¢ches
  - `num_links` : nombre de liens (blocks, is blocked by)
  - `has_attachments` : boolÃ©en
- [ ] Features catÃ©gorielles :
  - `issue_type` : Bug, Story, Task, etc.
  - `priority` : Highest, High, Medium, Low, Lowest
  - `component_primary` : premier composant (ou "None")
- [ ] Features temporelles :
  - `created_day_of_week` : 0-6
  - `created_hour` : 0-23
  - `sprint_day_created` : jour dans le sprint (1-14)
- [ ] Features dÃ©veloppeur (agrÃ©gÃ©es/pseudonymisÃ©es) :
  - `assignee_avg_cycle_time_30d` : moyenne cycle time assignee sur 30j
  - `assignee_completion_rate_30d` : taux de complÃ©tion sur 30j
  - `assignee_current_wip` : tickets en cours actuellement

**US2.2 â€” Features dÃ©veloppeur**
> En tant que data scientist, je veux des features sur la charge et performance des dÃ©veloppeurs.

CritÃ¨res d'acceptation :
- [ ] Module `developer_features.py` avec classe `DeveloperFeatureExtractor`
- [ ] MÃ©triques par dÃ©veloppeur (pseudonymisÃ©) :
  - `total_story_points_30d` : points complÃ©tÃ©s sur 30 jours
  - `avg_cycle_time_30d` : moyenne cycle time
  - `wip_count` : tickets en cours (status "In Progress" ou Ã©quivalent)
  - `worklog_hours_7d` : heures loguÃ©es sur 7 jours
  - `tickets_completed_7d` : tickets rÃ©solus sur 7 jours
  - `overdue_tickets` : tickets en retard (si due date)
- [ ] Indicateur de charge relative : Ã©cart Ã  la moyenne Ã©quipe
- [ ] Flag `at_risk` si workload > 1.3Ã— moyenne ou WIP > 5

**US2.3 â€” Features sprint**
> En tant que data scientist, je veux des features sur la santÃ© du sprint en cours.

CritÃ¨res d'acceptation :
- [ ] Module `sprint_features.py` avec classe `SprintFeatureExtractor`
- [ ] MÃ©triques sprint :
  - `days_elapsed` / `days_remaining`
  - `points_committed` : story points au dÃ©but du sprint
  - `points_completed` : story points done
  - `points_remaining` : points non terminÃ©s
  - `completion_rate` : completed / committed
  - `expected_completion_rate` : days_elapsed / total_days
  - `scope_creep_ratio` : points ajoutÃ©s mid-sprint / committed
  - `blocked_tickets_count` : tickets avec status "Blocked" ou flag
  - `velocity_vs_average` : ratio vs moyenne 5 derniers sprints
- [ ] Burndown thÃ©orique vs rÃ©el (points restants par jour)

**US2.4 â€” Pipeline feature engineering**
> En tant que data scientist, je veux un pipeline reproductible pour gÃ©nÃ©rer les features.

CritÃ¨res d'acceptation :
- [ ] Module `pipeline.py` avec classe `FeaturePipeline`
- [ ] MÃ©thode `build_ticket_training_set()` : gÃ©nÃ¨re DataFrame pour entraÃ®nement
- [ ] MÃ©thode `build_sprint_features(sprint_id)` : features sprint temps rÃ©el
- [ ] MÃ©thode `build_developer_features()` : features tous dÃ©veloppeurs
- [ ] Gestion des valeurs manquantes (imputation ou flag)
- [ ] Encodage catÃ©goriel compatible LightGBM (category dtype)
- [ ] Sauvegarde des features dans DuckDB (table `ml_features`)

**US2.5 â€” Target variable pour estimation**
> En tant que data scientist, je veux dÃ©finir clairement ce que je prÃ©dis.

CritÃ¨res d'acceptation :
- [ ] Target principale : `actual_cycle_time_hours` (temps en heures entre premier "In Progress" et "Done")
- [ ] Filtrage : uniquement tickets rÃ©solus, avec changelog complet
- [ ] Exclusion : tickets < 1h (probablement mal trackÃ©s) et > 500h (outliers)
- [ ] Alternative : `actual_lead_time_hours` pour comparaison

**US2.6 â€” ModÃ¨le baseline**
> En tant que data scientist, je veux un modÃ¨le baseline simple pour Ã©tablir une rÃ©fÃ©rence.

CritÃ¨res d'acceptation :
- [ ] Module `ticket_estimator.py` avec classe `TicketEstimator`
- [ ] Baseline 1 : moyenne par `issue_type` (MAE baseline)
- [ ] Baseline 2 : `LinearRegression` sur features numÃ©riques uniquement
- [ ] MÃ©triques : MAE, RMSE, RÂ², MAPE
- [ ] Split temporel (pas random !) : train sur tickets avant date X, test aprÃ¨s

**US2.7 â€” ModÃ¨le LightGBM**
> En tant que data scientist, je veux un modÃ¨le performant avec features catÃ©gorielles.

CritÃ¨res d'acceptation :
- [ ] ImplÃ©mentation `LGBMRegressor` dans `TicketEstimator`
- [ ] HyperparamÃ¨tres initiaux raisonnables :
  ```python
  params = {
      'objective': 'regression',
      'metric': 'mae',
      'num_leaves': 31,
      'learning_rate': 0.05,
      'feature_fraction': 0.8,
      'bagging_fraction': 0.8,
      'bagging_freq': 5,
      'verbose': -1
  }
  ```
- [ ] Cross-validation temporelle (TimeSeriesSplit, 5 folds)
- [ ] Feature importance extraction et logging
- [ ] Comparaison vs baselines

**US2.8 â€” Trainer et persistance**
> En tant que dÃ©veloppeur, je veux entraÃ®ner et sauvegarder les modÃ¨les facilement.

CritÃ¨res d'acceptation :
- [ ] Module `trainer.py` avec classe `ModelTrainer`
- [ ] MÃ©thode `train(model_type, config)` : entraÃ®nement + Ã©valuation
- [ ] Sauvegarde modÃ¨le : `models/ticket_estimator.pkl` (joblib)
- [ ] Sauvegarde metadata : `models/metadata.json` avec date, mÃ©triques, config
- [ ] Commande CLI : `python -m src.models.trainer --model ticket_estimator`
- [ ] Ajout au Makefile : `make train`

**US2.9 â€” PrÃ©diction sur nouveaux tickets**
> En tant qu'utilisateur, je veux obtenir une estimation pour un ticket donnÃ©.

CritÃ¨res d'acceptation :
- [ ] MÃ©thode `TicketEstimator.predict(issue_key)` : retourne estimation en heures
- [ ] Chargement automatique du modÃ¨le depuis pickle
- [ ] Intervalle de confiance (si possible via quantile regression ou bootstrap)
- [ ] Commande CLI : `python -m src.models.ticket_estimator predict PROJ-123`

#### Livrables Sprint 2
- [ ] Pipeline feature engineering complet
- [ ] ModÃ¨le `ticket_estimator` entraÃ®nÃ© et Ã©valuÃ©
- [ ] Notebook `02_feature_engineering.ipynb` documentant les choix
- [ ] Comparaison MAE modÃ¨le vs baseline (objectif : -15%)
- [ ] `make train` fonctionnel

#### Orientations techniques Sprint 2

**Split temporel obligatoire** :
```python
from sklearn.model_selection import TimeSeriesSplit
# Trier par date de crÃ©ation avant split
df_sorted = df.sort_values('created')
tscv = TimeSeriesSplit(n_splits=5)
```

**LightGBM avec catÃ©gorielles** :
```python
# Convertir en category dtype AVANT fit
for col in categorical_cols:
    df[col] = df[col].astype('category')
model.fit(X, y, categorical_feature=categorical_cols)
```

**Gestion des outliers** :
```python
# Winsorization plutÃ´t que suppression
from scipy.stats import mstats
y_clean = mstats.winsorize(y, limits=[0.01, 0.01])
```

---

### ðŸƒ SPRINT 3 : DÃ©tection Risques & Alertes
**Semaines 5-6**

#### Objectif Sprint
ImplÃ©menter la dÃ©tection de risques sprint et les alertes de charge dÃ©veloppeur. Valider les hypothÃ¨ses H2 et H3.

#### User Stories

**US3.1 â€” Score de risque sprint**
> En tant que manager, je veux un score 0-100 indiquant le risque de retard du sprint.

CritÃ¨res d'acceptation :
- [ ] Module `sprint_risk.py` avec classe `SprintRiskScorer`
- [ ] Score composite basÃ© sur :
  - `completion_gap` : Ã©cart entre progression rÃ©elle et thÃ©orique (poids 30%)
  - `velocity_ratio` : vÃ©locitÃ© actuelle vs historique (poids 25%)
  - `blocked_ratio` : proportion de tickets bloquÃ©s (poids 20%)
  - `scope_creep` : ajouts mid-sprint (poids 15%)
  - `days_remaining_factor` : urgence croissante (poids 10%)
- [ ] Seuils : 0-30 (vert), 31-60 (orange), 61-100 (rouge)
- [ ] Explication textuelle des facteurs contribuant au score

**US3.2 â€” ModÃ¨le prÃ©dictif risque sprint**
> En tant que data scientist, je veux un modÃ¨le ML pour prÃ©dire si le sprint sera complÃ©tÃ©.

CritÃ¨res d'acceptation :
- [ ] Target : `sprint_completed_on_time` (binaire, basÃ© sur historique)
- [ ] Features : mÃ©triques sprint Ã  mi-parcours (jour 7)
- [ ] ModÃ¨le : `LGBMClassifier` ou `RandomForestClassifier`
- [ ] MÃ©triques : Precision, Recall, F1, AUC-ROC
- [ ] Calibration des probabilitÃ©s pour score interprÃ©table

**US3.3 â€” Score de charge dÃ©veloppeur**
> En tant que manager, je veux voir la charge de chaque dÃ©veloppeur par rapport Ã  l'Ã©quipe.

CritÃ¨res d'acceptation :
- [ ] Module `workload_scorer.py` avec classe `WorkloadScorer`
- [ ] Score relatif : (charge_individu / moyenne_Ã©quipe) Ã— 100
- [ ] Composantes de la charge :
  - Story points en cours (WIP)
  - Heures loguÃ©es sur 7 jours
  - Nombre de tickets assignÃ©s non rÃ©solus
  - Tickets en retard (overdue)
- [ ] Seuils d'alerte : > 130% (surchargÃ©), < 70% (sous-utilisÃ©)
- [ ] Affichage pseudonymisÃ© par dÃ©faut

**US3.4 â€” DÃ©tection anomalies burn-out**
> En tant que manager, je veux dÃ©tecter les signaux de surcharge prolongÃ©e.

CritÃ¨res d'acceptation :
- [ ] Indicateurs de risque burn-out (agrÃ©gÃ©s Ã©quipe, pas individuel pour MVP) :
  - `overtime_ratio` : heures loguÃ©es / heures standard (>1.2 = alerte)
  - `weekend_work_frequency` : worklogs samedi/dimanche
  - `velocity_decline_trend` : pente nÃ©gative sur 4 semaines
  - `wip_sustained_high` : WIP > 5 pendant > 5 jours
- [ ] Score de santÃ© Ã©quipe (pas individuel)
- [ ] Alertes au niveau Ã©quipe, pas nominatives

**US3.5 â€” GÃ©nÃ©rateur d'alertes**
> En tant que manager, je veux recevoir des alertes structurÃ©es sur les risques dÃ©tectÃ©s.

CritÃ¨res d'acceptation :
- [ ] Module `alert_generator.py` avec classe `AlertGenerator`
- [ ] Types d'alertes :
  - `SPRINT_AT_RISK` : score sprint > 60
  - `TEAM_OVERLOADED` : > 50% des devs surchargÃ©s
  - `BLOCKED_TICKETS` : > 20% des tickets du sprint bloquÃ©s
  - `VELOCITY_DECLINING` : vÃ©locitÃ© < 70% de la moyenne
- [ ] Format alerte : `{type, severity, message, details, suggested_actions}`
- [ ] Historisation des alertes dans DuckDB (table `alerts`)

**US3.6 â€” CLI pour risques et alertes**
> En tant qu'utilisateur, je veux consulter les risques via ligne de commande.

CritÃ¨res d'acceptation :
- [ ] Commande `python -m src.interface.cli risk sprint` : affiche score sprint actif
- [ ] Commande `python -m src.interface.cli risk team` : affiche charge Ã©quipe
- [ ] Commande `python -m src.interface.cli alerts` : liste alertes actives
- [ ] Output formatÃ© avec `rich` (couleurs selon sÃ©vÃ©ritÃ©)
- [ ] Ajout au Makefile : `make risk`, `make alerts`

#### Livrables Sprint 3
- [ ] ModÃ¨le `sprint_risk` entraÃ®nÃ© et Ã©valuÃ©
- [ ] SystÃ¨me d'alertes fonctionnel
- [ ] CLI pour consultation risques
- [ ] Validation H2 : prÃ©cision alertes sprint > 70%
- [ ] Validation H3 : dÃ©tection Ã©carts charge > 30%

#### Orientations techniques Sprint 3

**Score composite avec poids** :
```python
def compute_risk_score(features):
    weights = {
        'completion_gap': 0.30,
        'velocity_ratio': 0.25,
        'blocked_ratio': 0.20,
        'scope_creep': 0.15,
        'urgency': 0.10
    }
    score = sum(w * normalize(features[k]) for k, w in weights.items())
    return min(100, max(0, score * 100))
```

**Rich pour CLI colorÃ©e** :
```python
from rich.console import Console
from rich.table import Table
console = Console()
# Rouge/orange/vert selon sÃ©vÃ©ritÃ©
console.print(f"[red]ALERT:[/red] {message}")
```

---

### ðŸƒ SPRINT 4 : Intelligence LLM & Interface Conversationnelle
**Semaines 7-8**

#### Objectif Sprint
IntÃ©grer le LLM local (Ollama) pour l'analyse conversationnelle et les recommandations. DÃ©but de l'interface Streamlit.

#### User Stories

**US4.1 â€” Client Ollama**
> En tant que dÃ©veloppeur, je veux une interface simple pour communiquer avec le LLM local.

CritÃ¨res d'acceptation :
- [ ] Module `llm_client.py` avec classe `OllamaClient`
- [ ] Configuration : modÃ¨le (`llama3.1:8b`), tempÃ©rature, max_tokens
- [ ] MÃ©thode `chat(messages)` : format OpenAI-compatible
- [ ] MÃ©thode `complete(prompt)` : complÃ©tion simple
- [ ] Gestion timeout et retry
- [ ] Fallback message si Ollama non disponible
- [ ] Support streaming (optionnel mais nice-to-have)

**US4.2 â€” Templates de prompts**
> En tant que dÃ©veloppeur, je veux des prompts structurÃ©s et rÃ©utilisables.

CritÃ¨res d'acceptation :
- [ ] Module `prompts.py` avec templates :
  - `SPRINT_ANALYSIS_PROMPT` : analyse Ã©tat du sprint
  - `TICKET_ESTIMATION_PROMPT` : expliquer une estimation
  - `WORKLOAD_ANALYSIS_PROMPT` : analyser charge Ã©quipe
  - `RECOMMENDATION_PROMPT` : gÃ©nÃ©rer suggestions d'actions
  - `RELEASE_PLANNING_PROMPT` : aide Ã  la planification
- [ ] Injection de donnÃ©es structurÃ©es (JSON/YAML) dans les prompts
- [ ] System prompt dÃ©finissant le rÃ´le de l'assistant

**US4.3 â€” Analyste IA**
> En tant que manager, je veux que l'IA analyse mes donnÃ©es et m'explique la situation.

CritÃ¨res d'acceptation :
- [ ] Module `analyst.py` avec classe `JiraAnalyst`
- [ ] MÃ©thode `analyze_sprint(sprint_id)` : rÃ©sumÃ© + insights
- [ ] MÃ©thode `analyze_ticket(issue_key)` : analyse + estimation expliquÃ©e
- [ ] MÃ©thode `analyze_team_health()` : Ã©tat de l'Ã©quipe
- [ ] MÃ©thode `answer_question(question, context)` : Q&A libre sur les donnÃ©es
- [ ] Formatage des donnÃ©es Jira en contexte digestible pour le LLM

**US4.4 â€” Recommandations d'actions**
> En tant que manager, je veux des suggestions concrÃ¨tes pour rÃ©soudre les problÃ¨mes.

CritÃ¨res d'acceptation :
- [ ] Module `recommender.py` avec classe `ActionRecommender`
- [ ] Recommandations contextuelles :
  - Sprint Ã  risque â†’ actions pour le sauver
  - DÃ©veloppeur surchargÃ© â†’ rÃ©assignations suggÃ©rÃ©es
  - Ticket bloquÃ© â†’ escalade ou dÃ©blocage
  - Backlog mal priorisÃ© â†’ re-priorisation
- [ ] Format : `{action, priority, rationale, effort, impact}`
- [ ] Scoring des recommandations par pertinence

**US4.5 â€” Interface CLI conversationnelle**
> En tant qu'utilisateur, je veux dialoguer avec l'IA depuis le terminal.

CritÃ¨res d'acceptation :
- [ ] Mode interactif : `python -m src.interface.cli chat`
- [ ] Historique de conversation maintenu
- [ ] Commandes spÃ©ciales : `/sprint`, `/ticket PROJ-123`, `/team`, `/quit`
- [ ] Affichage streaming de la rÃ©ponse (si supportÃ©)
- [ ] Ctrl+C pour interrompre proprement

**US4.6 â€” Dashboard Streamlit - Setup**
> En tant qu'utilisateur, je veux une interface visuelle basique pour voir les mÃ©triques.

CritÃ¨res d'acceptation :
- [ ] Module `dashboard.py` avec app Streamlit
- [ ] Page d'accueil avec :
  - Score de risque sprint (gauge ou progress bar)
  - MÃ©triques clÃ©s : points restants, jours restants, vÃ©locitÃ©
  - Liste des alertes actives
- [ ] Sidebar : sÃ©lection projet, sprint, pÃ©riode
- [ ] Commande : `make dashboard` â†’ `streamlit run src/interface/dashboard.py`

**US4.7 â€” Dashboard - Vue Ã©quipe**
> En tant que manager, je veux voir la charge de l'Ã©quipe visuellement.

CritÃ¨res d'acceptation :
- [ ] Page "Ã‰quipe" dans Streamlit
- [ ] Bar chart : charge par dÃ©veloppeur (pseudonymisÃ©)
- [ ] Heatmap : activitÃ© par jour de la semaine
- [ ] Tableau : WIP, points complÃ©tÃ©s, alertes par personne
- [ ] Toggle pour afficher/masquer les noms (mode anonyme)

**US4.8 â€” Dashboard - Chat intÃ©grÃ©**
> En tant qu'utilisateur, je veux dialoguer avec l'IA dans le dashboard.

CritÃ¨res d'acceptation :
- [ ] Page "Assistant" ou sidebar chat
- [ ] Input texte + historique conversation
- [ ] Boutons rapides : "Analyse sprint", "Ã‰tat Ã©quipe", "Prochaines actions"
- [ ] Affichage Markdown des rÃ©ponses

#### Livrables Sprint 4
- [ ] IntÃ©gration Ollama fonctionnelle
- [ ] Chat CLI interactif
- [ ] Dashboard Streamlit v1 (basique mais fonctionnel)
- [ ] Notebook `03_model_experiments.ipynb` avec analyses LLM
- [ ] `make dashboard` et `make chat` fonctionnels

#### Orientations techniques Sprint 4

**Format OpenAI pour Ollama** :
```python
from openai import OpenAI
client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')
response = client.chat.completions.create(
    model='llama3.1:8b',
    messages=[
        {"role": "system", "content": "Tu es un assistant expert en gestion de projet..."},
        {"role": "user", "content": "Analyse ce sprint..."}
    ]
)
```

**Contexte structurÃ© pour le LLM** :
```python
context = f"""
## Sprint actuel: {sprint.name}
- Jours restants: {days_remaining}
- Points complÃ©tÃ©s: {completed}/{committed}
- Score de risque: {risk_score}/100

## Tickets bloquÃ©s:
{blocked_tickets_summary}

## Charge Ã©quipe:
{team_workload_summary}
"""
```

**Streamlit minimal** :
```python
import streamlit as st
st.set_page_config(page_title="Jira Co-pilot", layout="wide")
st.metric("Risk Score", f"{risk_score}/100", delta=delta_vs_yesterday)
```

---

### ðŸƒ SPRINT 5 : Load Balancing & Release Planning
**Semaines 9-10**

#### Objectif Sprint
ImplÃ©menter les suggestions de rÃ©assignation (load balancing) et la gÃ©nÃ©ration de plans de release. Valider l'hypothÃ¨se H4.

#### User Stories

**US5.1 â€” Algorithme de load balancing**
> En tant que manager, je veux des suggestions de rÃ©assignation pour Ã©quilibrer la charge.

CritÃ¨res d'acceptation :
- [ ] Module `load_balancer.py` avec classe `LoadBalancer`
- [ ] Input : tickets Ã  rÃ©assigner (optionnel), contraintes (skills, prÃ©fÃ©rences)
- [ ] Output : liste de `{ticket, from_dev, to_dev, reason, confidence}`
- [ ] Algorithme :
  1. Identifier les devs surchargÃ©s (> 130% charge moyenne)
  2. Identifier les devs disponibles (< 80% charge moyenne)
  3. Matcher tickets selon : composant/skill, historique succÃ¨s, charge rÃ©sultante
- [ ] Respecter les contraintes : ne pas surcharger le receveur
- [ ] Score de confiance basÃ© sur la qualitÃ© du match

**US5.2 â€” Suggestions de rÃ©assignation sprint**
> En tant que manager, je veux voir les rÃ©assignations suggÃ©rÃ©es pour le sprint en cours.

CritÃ¨res d'acceptation :
- [ ] MÃ©thode `suggest_sprint_rebalancing(sprint_id)`
- [ ] Prioriser les tickets non commencÃ©s
- [ ] Ã‰viter de rÃ©assigner les tickets presque terminÃ©s
- [ ] Afficher l'impact projetÃ© sur le score de risque
- [ ] Export en format actionnable (CSV ou JSON)

**US5.3 â€” Planificateur de release**
> En tant que manager, je veux gÃ©nÃ©rer un plan de release optimisÃ©.

CritÃ¨res d'acceptation :
- [ ] Module `release_planner.py` avec classe `ReleasePlanner`
- [ ] Input : backlog (tickets), date cible, capacitÃ© Ã©quipe
- [ ] Output : plan de release avec sprints suggÃ©rÃ©s
- [ ] Algorithme :
  1. Estimer la durÃ©e de chaque ticket (via modÃ¨le ML)
  2. Calculer la capacitÃ© par sprint (vÃ©locitÃ© historique)
  3. Affecter les tickets par prioritÃ© + dÃ©pendances
  4. Optimiser pour minimiser le time-to-value
- [ ] Gestion des dÃ©pendances (ticket A doit Ãªtre fait avant B)

**US5.4 â€” Contraintes de release**
> En tant que manager, je veux pouvoir spÃ©cifier des contraintes pour le plan.

CritÃ¨res d'acceptation :
- [ ] Contraintes supportÃ©es :
  - `must_include` : tickets obligatoires dans la release
  - `deadline` : date limite hard
  - `max_risk` : niveau de risque acceptable
  - `team_availability` : absences planifiÃ©es
- [ ] Validation des contraintes (infaisabilitÃ© dÃ©tectÃ©e)
- [ ] Mode "what-if" : simuler diffÃ©rents scÃ©narios

**US5.5 â€” GÃ©nÃ©ration plan via LLM**
> En tant que manager, je veux que l'IA m'explique et affine le plan de release.

CritÃ¨res d'acceptation :
- [ ] Prompt structurÃ© avec : backlog, contraintes, capacitÃ©
- [ ] LLM gÃ©nÃ¨re : justification des choix, risques identifiÃ©s, alternatives
- [ ] Dialogue itÃ©ratif : "Et si on ajoutait ce ticket ?"
- [ ] Export du plan final en Markdown ou CSV

**US5.6 â€” Dashboard - Vue Load Balancing**
> En tant que manager, je veux visualiser les suggestions de rÃ©assignation.

CritÃ¨res d'acceptation :
- [ ] Page "Load Balancing" dans Streamlit
- [ ] Visualisation : barre de charge actuelle vs projetÃ©e
- [ ] Tableau des rÃ©assignations suggÃ©rÃ©es
- [ ] Bouton "Appliquer" (pour l'instant : export CSV, pas d'Ã©criture Jira)
- [ ] Filtres : par sÃ©vÃ©ritÃ©, par dÃ©veloppeur

**US5.7 â€” Dashboard - Vue Release Planning**
> En tant que manager, je veux planifier mes releases visuellement.

CritÃ¨res d'acceptation :
- [ ] Page "Release Planning" dans Streamlit
- [ ] Input : sÃ©lection tickets du backlog, date cible
- [ ] Output : timeline des sprints avec tickets assignÃ©s
- [ ] Indicateurs : probabilitÃ© de succÃ¨s, buffer disponible
- [ ] Export du plan en Markdown/CSV

**US5.8 â€” Backlog priorization assistant**
> En tant que manager, je veux de l'aide pour prioriser mon backlog.

CritÃ¨res d'acceptation :
- [ ] Score de priorisation par ticket :
  - `business_value` : estimÃ© ou taggÃ© (High/Medium/Low)
  - `effort` : estimation ML
  - `dependencies_impact` : combien de tickets dÃ©bloquÃ©s
  - `age` : depuis combien de temps dans le backlog
- [ ] Ranking WSJF-like : (value + urgency) / effort
- [ ] Suggestions LLM : "Ces 5 tickets devraient Ãªtre priorisÃ©s parce que..."

#### Livrables Sprint 5
- [ ] Algorithme load balancing fonctionnel
- [ ] Release planner avec export
- [ ] Dashboard pages load balancing et release
- [ ] Validation H4 : temps de planification rÃ©duit (mesure qualitative)
- [ ] `make plan-release` fonctionnel

#### Orientations techniques Sprint 5

**Algorithme simple de bin packing pour release** :
```python
def assign_to_sprints(tickets, sprint_capacity):
    sprints = []
    current_sprint = []
    current_load = 0
    
    for ticket in sorted(tickets, key=lambda t: -t['priority']):
        if current_load + ticket['estimate'] <= sprint_capacity:
            current_sprint.append(ticket)
            current_load += ticket['estimate']
        else:
            sprints.append(current_sprint)
            current_sprint = [ticket]
            current_load = ticket['estimate']
    
    if current_sprint:
        sprints.append(current_sprint)
    
    return sprints
```

**Gestion des dÃ©pendances (tri topologique)** :
```python
from collections import deque

def topological_sort(tickets, dependencies):
    # dependencies: dict {ticket_key: [blocked_by_keys]}
    # Retourne tickets ordonnÃ©s avec dÃ©pendances respectÃ©es
    ...
```

---

### ðŸƒ SPRINT 6 : Consolidation, Tests & Documentation
**Semaines 11-12**

#### Objectif Sprint
Stabiliser le MVP, ajouter les tests manquants, documenter et prÃ©parer pour une utilisation quotidienne.

#### User Stories

**US6.1 â€” Tests unitaires complets**
> En tant que dÃ©veloppeur, je veux une couverture de tests suffisante pour les modules critiques.

CritÃ¨res d'acceptation :
- [ ] Couverture > 60% sur les modules `src/`
- [ ] Tests pour :
  - `jira_client/` : mock des appels API
  - `features/` : validation des calculs
  - `models/` : prÃ©dictions sur donnÃ©es connues
  - `intelligence/` : mock du LLM
- [ ] Fixtures pytest dans `conftest.py` avec donnÃ©es de test
- [ ] `make test` passe sans erreur

**US6.2 â€” Tests d'intÃ©gration**
> En tant que dÃ©veloppeur, je veux valider les flux end-to-end.

CritÃ¨res d'acceptation :
- [ ] Test E2E : sync â†’ features â†’ prediction
- [ ] Test E2E : sprint features â†’ risk score â†’ alerts
- [ ] Test avec base DuckDB de test (pas la prod)
- [ ] DonnÃ©es de test rÃ©alistes (anonymisÃ©es de vraies donnÃ©es)

**US6.3 â€” Gestion des erreurs robuste**
> En tant qu'utilisateur, je veux des messages d'erreur clairs et une app qui ne crashe pas.

CritÃ¨res d'acceptation :
- [ ] Try/except appropriÃ©s dans tous les modules
- [ ] Messages d'erreur user-friendly (pas de stacktraces brutes)
- [ ] Logging structurÃ© avec `loguru` (fichier + console)
- [ ] Graceful degradation : si Ollama down, fonctionnalitÃ©s ML marchent encore

**US6.4 â€” Configuration flexible**
> En tant qu'utilisateur, je veux pouvoir configurer l'outil sans modifier le code.

CritÃ¨res d'acceptation :
- [ ] Toute la config dans `.env` et `config/*.yaml`
- [ ] Validation des configs au dÃ©marrage (Pydantic)
- [ ] Valeurs par dÃ©faut sensÃ©es
- [ ] Documentation des options de config dans README

**US6.5 â€” Documentation technique**
> En tant que dÃ©veloppeur futur, je veux comprendre comment le projet fonctionne.

CritÃ¨res d'acceptation :
- [ ] README complet :
  - Installation (prÃ©requis, setup)
  - Configuration (Jira, Ollama)
  - Utilisation (commandes principales)
  - Architecture (schÃ©ma simplifiÃ©)
- [ ] Docstrings sur les classes et mÃ©thodes publiques
- [ ] `ARCHITECTURE.md` : dÃ©cisions techniques et rationale
- [ ] `CHANGELOG.md` : historique des versions

**US6.6 â€” Dashboard polish**
> En tant qu'utilisateur, je veux un dashboard utilisable au quotidien.

CritÃ¨res d'acceptation :
- [ ] Navigation cohÃ©rente entre les pages
- [ ] Indicateurs de chargement (spinners)
- [ ] Refresh automatique des donnÃ©es (bouton ou timer)
- [ ] Responsive basique (utilisable sur laptop)
- [ ] ThÃ¨me cohÃ©rent (dark mode optionnel)

**US6.7 â€” Automatisation sync**
> En tant qu'utilisateur, je veux que les donnÃ©es se synchronisent automatiquement.

CritÃ¨res d'acceptation :
- [ ] APScheduler configurÃ© pour sync toutes les 30 minutes
- [ ] Persistence du scheduler (survit aux redÃ©marrages)
- [ ] Option de sync manuel dans le dashboard
- [ ] Indicateur de fraÃ®cheur des donnÃ©es ("DerniÃ¨re sync : il y a 15 min")

**US6.8 â€” Export et rapports**
> En tant que manager, je veux exporter des rapports pour les partager.

CritÃ¨res d'acceptation :
- [ ] Export sprint report en Markdown
- [ ] Export team workload en CSV
- [ ] Export release plan en Markdown
- [ ] Export alertes en JSON
- [ ] Boutons d'export dans le dashboard

**US6.9 â€” Mode dÃ©mo / donnÃ©es de test**
> En tant que dÃ©veloppeur, je veux pouvoir dÃ©montrer l'outil sans donnÃ©es rÃ©elles.

CritÃ¨res d'acceptation :
- [ ] Script `generate_demo_data.py` : gÃ©nÃ¨re donnÃ©es fictives rÃ©alistes
- [ ] Flag `--demo` pour lancer avec donnÃ©es de dÃ©mo
- [ ] DonnÃ©es de dÃ©mo : 200 tickets, 10 devs, 6 mois d'historique

**US6.10 â€” Validation finale hypothÃ¨ses**
> En tant que product owner, je veux mesurer le succÃ¨s du MVP.

CritÃ¨res d'acceptation :
- [ ] H1 (Estimation) : Comparer MAE modÃ¨le vs estimations humaines sur 20 tickets
- [ ] H2 (Risque sprint) : VÃ©rifier rÃ©trospectivement sur 3 sprints passÃ©s
- [ ] H3 (Charge Ã©quipe) : Confirmer dÃ©tection des dÃ©sÃ©quilibres connus
- [ ] H4 (Release planning) : Mesurer temps de planification avant/aprÃ¨s
- [ ] Document `VALIDATION.md` avec rÃ©sultats

#### Livrables Sprint 6
- [ ] Suite de tests complÃ¨te (> 60% coverage)
- [ ] Documentation complÃ¨te (README, ARCHITECTURE)
- [ ] Dashboard stable et utilisable
- [ ] Sync automatisÃ©
- [ ] Rapport de validation des hypothÃ¨ses
- [ ] **MVP COMPLET ET VALIDÃ‰**

#### Orientations techniques Sprint 6

**Structure tests avec pytest** :
```python
# tests/conftest.py
import pytest
import duckdb

@pytest.fixture
def test_db():
    conn = duckdb.connect(':memory:')
    # Setup schema
    yield conn
    conn.close()

@pytest.fixture
def mock_jira_client(mocker):
    return mocker.patch('src.jira_client.fetcher.JiraFetcher')
```

**Logging avec loguru** :
```python
from loguru import logger

logger.add("logs/app.log", rotation="10 MB", retention="7 days")
logger.info("Sync started", project="PROJ", mode="incremental")
```

---

## ANNEXES

### A. Commandes Makefile finales

```makefile
.PHONY: install test lint sync train predict risk alerts chat dashboard

install:
	pip install -e ".[dev]"

test:
	pytest tests/ -v --cov=src --cov-report=term-missing

lint:
	ruff check src/ tests/
	ruff format src/ tests/

sync:
	python -m src.jira_client.sync --mode incremental

sync-full:
	python -m src.jira_client.sync --mode full

train:
	python -m src.models.trainer --all

predict:
	python -m src.models.ticket_estimator predict $(TICKET)

risk:
	python -m src.interface.cli risk sprint

alerts:
	python -m src.interface.cli alerts

chat:
	python -m src.interface.cli chat

dashboard:
	streamlit run src/interface/dashboard.py

demo:
	python scripts/generate_demo_data.py
	$(MAKE) dashboard
```

### B. Variables d'environnement (.env.example)

```env
# Jira Configuration
JIRA_URL=https://your-instance.atlassian.net
JIRA_EMAIL=your-email@company.com
JIRA_API_TOKEN=your-api-token
JIRA_PROJECT_KEY=PROJ
JIRA_BOARD_ID=1

# Custom Fields Mapping (optionnel, override dans jira_config.yaml)
JIRA_STORY_POINTS_FIELD=customfield_10016

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# Application Settings
LOG_LEVEL=INFO
SYNC_INTERVAL_MINUTES=30
ANONYMIZE_DEVELOPERS=true

# Database
DATABASE_PATH=data/jira.duckdb
```

### C. CritÃ¨res de succÃ¨s MVP

| MÃ©trique | Cible | MÃ©thode de mesure |
|----------|-------|-------------------|
| MAE estimation vs baseline | -15% | Comparaison sur 50 tickets test |
| PrÃ©cision alertes sprint | > 70% | Validation rÃ©tro sur 5 sprints |
| DÃ©tection dÃ©sÃ©quilibre charge | > 30% Ã©cart | Comparaison avec perception manager |
| Temps planification release | -50% | Mesure avant/aprÃ¨s sur 1 release |
| Temps daily review | -30% | Mesure subjective |
| Satisfaction utilisateur | > 4/5 | Auto-Ã©valuation |

### D. Ã‰volutions post-MVP (backlog futur)

1. **IntÃ©gration bidirectionnelle Jira** : crÃ©er/modifier tickets depuis l'outil
2. **Notifications Slack/Teams** : alertes en temps rÃ©el
3. **Multi-projets** : gÃ©rer plusieurs projets Jira
4. **Embeddings pour similaritÃ©** : trouver tickets similaires historiques
5. **MCP Anthropic** : intÃ©gration temps rÃ©el via protocol
6. **DÃ©ploiement cloud** : version hÃ©bergÃ©e
7. **API REST** : exposer les fonctionnalitÃ©s pour intÃ©grations
8. **Fine-tuning LLM** : amÃ©liorer les rÃ©ponses sur le domaine spÃ©cifique

---

## INSTRUCTIONS POUR CLAUDE OPUS 4.5

Tu es le dÃ©veloppeur principal de ce projet. Ton rÃ´le est d'implÃ©menter chaque sprint de maniÃ¨re autonome, en respectant :

1. **La stack technique imposÃ©e** â€” Ne propose pas d'alternatives sauf si bloquÃ©
2. **La structure de projet** â€” Respecte l'arborescence dÃ©finie
3. **Les critÃ¨res d'acceptation** â€” Chaque US doit Ãªtre 100% complÃ©tÃ©e
4. **La philosophie MVP** â€” Fonctionnel > Parfait, mais pas de code sale
5. **Les bonnes pratiques** â€” Type hints, docstrings, tests

Ã€ chaque session de travail :
- Indique le sprint et l'US en cours
- Propose le code complet (pas de placeholders)
- Explique les choix techniques si pertinent
- Signale les blocages ou questions

**Let's build this! ðŸš€**